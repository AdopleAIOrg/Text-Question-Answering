{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNguBlC9n6GkKBjCAHe1CO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AdopleAIOrg/Text-Question-Answering/blob/main/Text_QA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSu-1hm1sSUV"
      },
      "outputs": [],
      "source": [
        "!pip install -r /content/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "from transformers import DistilBertTokenizer, TFDistilBertForQuestionAnswering\n",
        "import tensorflow as tf\n",
        "\n",
        "class TextQA:\n",
        "  \"\"\"\n",
        "  A class for performing question answering on text using the DistilBERT model.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "      \"\"\"\n",
        "        Initializes the TextQA class by loading the DistilBERT model and tokenizer.\n",
        "      \"\"\"\n",
        "\n",
        "      self.tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "      self.model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-cased-distilled-squad\")\n",
        "\n",
        "  def _find_answer(self, text: str, question: str) -> str:\n",
        "\n",
        "    \"\"\"\n",
        "    Tokenizes inputs and predicts the answer using the DistilBERT model.\n",
        "\n",
        "    Args:\n",
        "        text (str): The text to search for the answer.\n",
        "        question (str): The question related to the text.\n",
        "\n",
        "    Returns:\n",
        "        str: The predicted answer.\n",
        "    \"\"\"\n",
        "\n",
        "  # Tokenize inputs and predict answer\n",
        "    if text and question:\n",
        "        inputs = self.tokenizer(question, text, return_tensors=\"tf\")\n",
        "        outputs = self.model(**inputs)\n",
        "        answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
        "        answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n",
        "        predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
        "        predict_answer = self.tokenizer.decode(predict_answer_tokens)\n",
        "        return predict_answer\n",
        "\n",
        "  def streamlit_interface(self):\n",
        "    \"\"\"\n",
        "    Creates the Streamlit interface for the Question Answering application.\n",
        "    \"\"\"\n",
        "\n",
        "    # Display predicted answer\n",
        "    st.title(\"Question Answering \")\n",
        "\n",
        "    # Get text and question inputs from user\n",
        "    text = st.text_area(\"Enter the text:\")\n",
        "    question = st.text_input(\"Write the question:\")\n",
        "\n",
        "    final_answer = self._find_answer(text,question)\n",
        "    st.subheader(\"Predicted Answer:\")\n",
        "    st.write(final_answer)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "  text_qa = TextQA()\n",
        "  text_qa.streamlit_interface()"
      ],
      "metadata": {
        "id": "L8cwE0PbskbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "EnWELzUTuqmg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}